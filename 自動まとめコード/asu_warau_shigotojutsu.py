# ----------------------------------------------------------------
# ã¾ã¨ã‚ã‚µã‚¤ãƒˆè‡ªå‹•ç”Ÿæˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€æœ€çµ‚ãƒ‡ã‚¶ã‚¤ãƒ³å¯¾å¿œç‰ˆã€‘
# ----------------------------------------------------------------

# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’èª­ã¿è¾¼ã‚€
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import time

# ----------------------------------------------------------------
# 2. è¨­å®šé …ç›®ï¼ˆã“ã“ã‚’è‡ªç”±ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
# ----------------------------------------------------------------

# â˜…æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŒ‡å®š
SEARCH_KEYWORD = "æ˜æ—¥ã€€ã‚¢ãƒã‚¤ãƒ³ãƒˆç²å¾—ã€€ã‚³ãƒ„"

# â˜…æ¤œç´¢ã—ã¦æƒ…å ±ã‚’é›†ã‚ã‚‹ã‚µã‚¤ãƒˆã®æ•°
NUM_SITES_TO_CHECK = 10

# â˜…ç”Ÿæˆã•ã‚Œã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã«ä½œã‚‹ã‚ˆã†ã«è¨­å®šæ¸ˆã¿ï¼ï¼‰
OUTPUT_FILENAME = "matome/æ˜æ—¥ã¯ç¬‘ã†ä»•äº‹è¡“.html"

# â˜…å„ã‚µã‚¤ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹æœ¬æ–‡ã®æœ€å¤§æ–‡å­—æ•°
MAX_TEXT_LENGTH = 150

# ----------------------------------------------------------------
# ã“ã“ã‹ã‚‰ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯å¤‰æ›´ä¸è¦ã§ã™
# ----------------------------------------------------------------

def search_web(keyword, max_results):
    """æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§Webæ¤œç´¢ã—ã€URLã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    print(f"ã€Œ{keyword}ã€ã§Webæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")
    results = []
    try:
        with DDGS() as ddgs:
            for r in ddgs.text(keyword, region='jp-jp', max_results=max_results):
                results.append(r)
        print(f"{len(results)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return [r['href'] for r in results]
    except Exception as e:
        print(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

def scrape_content(url):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®è¦ç‚¹ã‚’æŠ½å‡ºã™ã‚‹"""
    print(f"-> {url} ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('title').text if soup.find('title') else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
        paragraphs = soup.find_all('p')
        content_text = ""
        for p in paragraphs:
            content_text += p.get_text().strip()
            if len(content_text) > MAX_TEXT_LENGTH:
                break
        if len(content_text) < 50:
             content_text = ' '.join(soup.body.get_text().split())
        summary = content_text[:MAX_TEXT_LENGTH] + "..." if len(content_text) > MAX_TEXT_LENGTH else content_text
        if not summary:
            return None
        return {'title': title.strip(), 'summary': summary.strip(), 'url': url}
    except Exception as e:
        print(f"  (ã‚¨ãƒ©ãƒ¼: {url} ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ - {e})")
        return None

def create_html(keyword, site_data_list):
    """æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰HTMLã‚’ç”Ÿæˆã™ã‚‹"""
    print("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    cards_html = ""
    for data in site_data_list:
        cards_html += f"""
            <div class="summary-card">
                <h2><a href="{data['url']}" target="_blank">{data['title']}</a></h2>
                <p>{data['summary']}</p>
                <a href="{data['url']}" target="_blank" class="source-link">ç¶šãã‚’èª­ã‚€</a>
            </div>
        """
        
    # â–¼â–¼â–¼ã€é‡è¦ã€‘ã“ã“ãŒã€æ–°ã—ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ â–¼â–¼â–¼
    html_template = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{keyword} ã®ã¾ã¨ã‚</title>
    
    <!-- å…±é€šã®ãƒ‡ã‚¶ã‚¤ãƒ³æŒ‡ç¤ºæ›¸ï¼ˆstyle.cssï¼‰ã‚’å‘¼ã³å‡ºã™ -->
    <link rel="stylesheet" href="../style.css">

    <!-- ã“ã®ãƒšãƒ¼ã‚¸ç‹¬è‡ªã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¿½åŠ  -->
    <style>
        .container {{
            max-width: 850px;
        }}
        .summary-card {{
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
            margin-bottom: 25px;
            padding: 25px;
            transition: transform 0.2s;
        }}
        .summary-card:hover {{
            transform: translateY(-3px);
            box-shadow: 0 6px 15px rgba(0,0,0,0.1);
        }}
        .summary-card h2 {{
            font-size: 1.4em;
            margin-top: 0;
            color: #3498db;
        }}
        .summary-card h2 a {{
            text-decoration: none;
            color: inherit;
        }}
        .summary-card p {{
            line-height: 1.7;
            margin-bottom: 15px;
        }}
        .source-link {{
            display: inline-block;
            font-weight: bold;
            color: #3498db;
            text-decoration: none;
        }}
        footer {{
            text-align: center;
            margin-top: 40px;
            color: #7f8c8d;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1 class="section-title" style="font-size: 2.2em; border-bottom: 3px solid #3498db; padding-bottom: 10px;">ã€Œ{keyword}ã€ã«ã¤ã„ã¦ã®è‡ªå‹•ã¾ã¨ã‚</h1>
        </header>
        <main>
            {cards_html}
        </main>
        
        <footer>
            <p>ã“ã®ãƒšãƒ¼ã‚¸ã¯Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
        </footer>

        <!-- æˆ»ã‚‹ãƒªãƒ³ã‚¯ã‚’åˆ†ã‹ã‚Šã‚„ã™ãè¨­ç½® -->
        <div class="back-link" style="text-align: center; margin-top: 40px; padding-top: 30px; border-top: 1px solid #eee;">
            <a href="./matome-list.html" class="back-button" style="margin-right: 20px;">Â« ã¾ã¨ã‚ä¸€è¦§ã«æˆ»ã‚‹</a>
            <a href="../index.html" class="back-button">Â« ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹</a>
        </div>
    </div>
</body>
</html>
    """
    return html_template

# --- ãƒ¡ã‚¤ãƒ³ã®å‡¦ç† ---
if __name__ == "__main__":
    urls = search_web(SEARCH_KEYWORD, NUM_SITES_TO_CHECK)
    all_site_data = []
    if urls:
        for url in urls:
            content_data = scrape_content(url)
            if content_data:
                all_site_data.append(content_data)
            time.sleep(1)
    if all_site_data:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆã™ã‚‹
        output_dir = os.path.dirname(OUTPUT_FILENAME)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ãƒ•ã‚©ãƒ«ãƒ€ '{output_dir}' ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        final_html = create_html(SEARCH_KEYWORD, all_site_data)
        with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
            f.write(final_html)
        print("-" * 50)
        print(f"ğŸ‰ æˆåŠŸï¼ ã€Œ{OUTPUT_FILENAME}ã€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
        
        # Seleniumã®éƒ¨åˆ†ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ç¢ºèªç”¨ãªã®ã§ã€ä»Šã¯ãã®ã¾ã¾ã§OKã§ã™ã€‚
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            import os
            print("ãƒ‡ãƒãƒƒã‚°ç”¨Chromeã«æ¥ç¶šã—ã¦ã€ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™...")
            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            driver = webdriver.Chrome(options=chrome_options)
            file_path = os.path.abspath(OUTPUT_FILENAME)
            file_url = 'file:///' + file_path.replace('\\', '/')
            driver.get(file_url)
            print("ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã—ãŸï¼")
        except Exception as e:
            print(f"ãƒ–ãƒ©ã‚¦ã‚¶ã®æ“ä½œã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°ç”¨ChromeãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        print("-" * 50)
    else:
        print("æœ‰åŠ¹ãªæƒ…å ±ã‚’åé›†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")